{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config = {'layer_specs': [784, 50, 50, 10], 'activation': 'tanh', 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 100, 'early_stop': True, 'early_stop_epoch': 5, 'L2_penalty': 0, 'momentum': True, 'momentum_gamma': 0.99}\n",
      "Shape of x_train = (60000, 784), y_train = (60000, 10)\n",
      "Shape of x_train = (48000, 784)\n",
      "Shape of y_train = (48000, 10)\n",
      "Shape of x_valid = (12000, 784)\n",
      "Shape of y_valid = (12000, 10)\n",
      "train: Epoch = 0, training loss = 5.747742319017559, training acc = 0.2918541666666667,         validation loss = 2.753177558304377, validation acc = 0.48433333333333334\n",
      "train: Epoch = 1, training loss = 2.0620826422438228, training acc = 0.53125,         validation loss = 1.700779755195751, validation acc = 0.5516666666666666\n",
      "train: Epoch = 2, training loss = 1.3976253652686637, training acc = 0.5811041666666666,         validation loss = 1.3281616384078814, validation acc = 0.5789166666666666\n",
      "train: Epoch = 3, training loss = 1.1714214612839433, training acc = 0.6083125,         validation loss = 1.1729491145795803, validation acc = 0.60025\n",
      "train: Epoch = 4, training loss = 1.0646388706196908, training acc = 0.6322916666666667,         validation loss = 1.1131263066608597, validation acc = 0.607\n",
      "train: Epoch = 5, training loss = 1.0328847293187193, training acc = 0.6421666666666667,         validation loss = 1.0862666156948224, validation acc = 0.6145\n",
      "train: Epoch = 6, training loss = 1.0169209656816462, training acc = 0.6459375,         validation loss = 1.0708833005822644, validation acc = 0.6191666666666666\n",
      "train: Epoch = 7, training loss = 0.9997534733135117, training acc = 0.6481458333333333,         validation loss = 1.066801097862646, validation acc = 0.6190833333333333\n",
      "train: Epoch = 8, training loss = 0.9927678954003042, training acc = 0.6517708333333333,         validation loss = 1.060038400999055, validation acc = 0.6163333333333333\n",
      "train: Epoch = 9, training loss = 1.0041716171037778, training acc = 0.6480833333333333,         validation loss = 1.056357695793409, validation acc = 0.6188333333333333\n",
      "train: Epoch = 10, training loss = 0.9976596478961034, training acc = 0.6490625,         validation loss = 1.0518621087829254, validation acc = 0.6231666666666666\n",
      "train: Epoch = 11, training loss = 0.986956540724994, training acc = 0.6501666666666667,         validation loss = 1.0509497247386146, validation acc = 0.6216666666666667\n",
      "train: Epoch = 12, training loss = 0.9892248076238703, training acc = 0.6503125,         validation loss = 1.053225393016784, validation acc = 0.6209166666666667\n",
      "train: Epoch = 13, training loss = 0.9899354129462963, training acc = 0.6513125,         validation loss = 1.049839056455393, validation acc = 0.6211666666666666\n",
      "train: Epoch = 14, training loss = 0.9866632178620958, training acc = 0.6504791666666667,         validation loss = 1.049630913194509, validation acc = 0.6188333333333333\n",
      "train: Epoch = 15, training loss = 0.9906831871787491, training acc = 0.6473333333333333,         validation loss = 1.0492890398419927, validation acc = 0.6215833333333334\n",
      "train: Epoch = 16, training loss = 0.9832593718910663, training acc = 0.6501875,         validation loss = 1.0486668317820682, validation acc = 0.6225\n",
      "train: Epoch = 17, training loss = 0.9864868350230149, training acc = 0.651875,         validation loss = 1.0490990586300606, validation acc = 0.6215\n",
      "train: Epoch = 18, training loss = 0.9821518624647406, training acc = 0.6536875,         validation loss = 1.0480749095187394, validation acc = 0.6265833333333334\n",
      "train: Epoch = 19, training loss = 0.9805495903346011, training acc = 0.650875,         validation loss = 1.0495179356710331, validation acc = 0.6238333333333334\n",
      "train: Epoch = 20, training loss = 0.9823792064722441, training acc = 0.6521458333333333,         validation loss = 1.0474154045350867, validation acc = 0.6245833333333334\n",
      "train: Epoch = 21, training loss = 0.9753981097369798, training acc = 0.6543958333333333,         validation loss = 1.0489112227242277, validation acc = 0.61975\n",
      "train: Epoch = 22, training loss = 0.9789662407244821, training acc = 0.6530416666666666,         validation loss = 1.0465329331658095, validation acc = 0.6228333333333333\n",
      "train: Epoch = 23, training loss = 0.9810246775160395, training acc = 0.65825,         validation loss = 1.0468696310849952, validation acc = 0.62875\n",
      "train: Epoch = 24, training loss = 0.9804775582625497, training acc = 0.6529791666666667,         validation loss = 1.0469786969676411, validation acc = 0.6228333333333333\n",
      "train: Epoch = 25, training loss = 0.9837266621252162, training acc = 0.6511666666666667,         validation loss = 1.0485533375551461, validation acc = 0.6229166666666667\n",
      "train: Epoch = 26, training loss = 0.9799789670050071, training acc = 0.6525416666666667,         validation loss = 1.0468018869031566, validation acc = 0.6233333333333333\n",
      "train: Epoch = 27, training loss = 0.9818472370873613, training acc = 0.6513958333333333,         validation loss = 1.0445281238716961, validation acc = 0.6265833333333334\n",
      "train: Epoch = 28, training loss = 0.9829364769494798, training acc = 0.6501666666666667,         validation loss = 1.0454633684977113, validation acc = 0.622\n",
      "train: Epoch = 29, training loss = 0.9774061257893191, training acc = 0.6548958333333333,         validation loss = 1.0471716074202841, validation acc = 0.6233333333333333\n",
      "train: Epoch = 30, training loss = 0.9677509272004327, training acc = 0.6581666666666667,         validation loss = 1.0439876543272137, validation acc = 0.6245833333333334\n",
      "train: Epoch = 31, training loss = 0.9755087595129829, training acc = 0.6545833333333333,         validation loss = 1.0462438150780613, validation acc = 0.6214166666666666\n",
      "train: Epoch = 32, training loss = 0.9789694673175955, training acc = 0.6545833333333333,         validation loss = 1.042472753495748, validation acc = 0.6255\n",
      "train: Epoch = 33, training loss = 0.9706951949920205, training acc = 0.6558125,         validation loss = 1.043764853831099, validation acc = 0.6256666666666667\n",
      "train: Epoch = 34, training loss = 0.975838935706923, training acc = 0.6528333333333334,         validation loss = 1.044296319111668, validation acc = 0.6223333333333333\n",
      "train: Epoch = 35, training loss = 0.9741296346392518, training acc = 0.6525,         validation loss = 1.0423382866153716, validation acc = 0.6250833333333333\n",
      "train: Epoch = 36, training loss = 0.9756593254401046, training acc = 0.6595208333333333,         validation loss = 1.0420722240021307, validation acc = 0.6238333333333334\n",
      "train: Epoch = 37, training loss = 0.9666064782119179, training acc = 0.6592083333333333,         validation loss = 1.0412403757186512, validation acc = 0.6250833333333333\n",
      "train: Epoch = 38, training loss = 0.9685255354441852, training acc = 0.6530208333333334,         validation loss = 1.039973934708458, validation acc = 0.6255\n",
      "train: Epoch = 39, training loss = 0.9729309552204631, training acc = 0.6569166666666667,         validation loss = 1.0394492679648417, validation acc = 0.6254166666666666\n",
      "train: Epoch = 40, training loss = 0.9621885981059083, training acc = 0.6604791666666666,         validation loss = 1.0383094157392536, validation acc = 0.6256666666666667\n",
      "train: Epoch = 41, training loss = 0.9654140481745594, training acc = 0.6583958333333333,         validation loss = 1.0402662762789738, validation acc = 0.6269166666666667\n",
      "train: Epoch = 42, training loss = 0.9659558726137397, training acc = 0.6556666666666666,         validation loss = 1.0396823907475512, validation acc = 0.6255\n",
      "train: Epoch = 43, training loss = 0.9744417200703868, training acc = 0.6572083333333333,         validation loss = 1.0391739950400203, validation acc = 0.6236666666666667\n",
      "train: Epoch = 44, training loss = 0.9682128867003624, training acc = 0.6592083333333333,         validation loss = 1.039464190856314, validation acc = 0.6265833333333334\n",
      "train: Epoch = 45, training loss = 0.9683100483468395, training acc = 0.6559166666666667,         validation loss = 1.0389386227650939, validation acc = 0.6251666666666666\n",
      "train: Epoch = 46, training loss = 0.966913249747077, training acc = 0.6591875,         validation loss = 1.038780991211174, validation acc = 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.615\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CSE 251B: Programming Assignment 2\n",
    "# Winter 2021\n",
    "################################################################################\n",
    "# To install PyYaml, refer to the instructions for your system:\n",
    "# https://pyyaml.org/wiki/PyYAMLDocumentation\n",
    "################################################################################\n",
    "# If you don't have NumPy installed, please use the instructions here:\n",
    "# https://scipy.org/install.html\n",
    "################################################################################\n",
    "\n",
    "import os, gzip\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Thus, we could load the configuration file from the given path\n",
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Load the configuration from config.yaml.\n",
    "    \"\"\"\n",
    "    return yaml.load(open('config.yaml', 'r'), Loader=yaml.SafeLoader)\n",
    "\n",
    "# Here, we normalize the samples in the given dataset\n",
    "def normalize_data(inp):\n",
    "    \"\"\"\n",
    "        TODO: Normalize your inputs here to have 0 mean and unit variance.\n",
    "    \"\"\"\n",
    "    normalized_inp = (inp - np.mean(inp,axis=0))/inp.std(axis=0)\n",
    "    return normalized_inp\n",
    "\n",
    "# Thus, we could compute the one hot encoding of the given dataset\n",
    "def one_hot_encoding(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "        TODO: Encode labels using one hot encoding and return them.\n",
    "    \"\"\"\n",
    "    numSamples = labels.shape[0]\n",
    "    Labels = np.zeros(shape=(numSamples, num_classes))\n",
    "    for i in range(numSamples):\n",
    "        Labels[i,labels[i]] = 1\n",
    "    return Labels\n",
    "\n",
    "def load_data(path, mode='train'):\n",
    "    \"\"\"\n",
    "        Load Fashion MNIST data.\n",
    "        Use mode='train' for train and mode='t10k' for test.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_path = os.path.join(path, f'{mode}-labels-idx1-ubyte.gz')\n",
    "    images_path = os.path.join(path, f'{mode}-images-idx3-ubyte.gz')\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    normalized_images = normalize_data(images)\n",
    "    one_hot_labels    = one_hot_encoding(labels, num_classes=10)\n",
    "\n",
    "    return normalized_images, one_hot_labels\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    TODO: Implement the softmax function here.\n",
    "    Remember to take care of the overflow condition.\n",
    "    \"\"\"\n",
    "    numerator = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "class Activation():\n",
    "    \"\"\"\n",
    "    The class implements different types of activation functions for\n",
    "    your neural network layers.\n",
    "\n",
    "    Example (for sigmoid):\n",
    "        >>> sigmoid_layer = Activation(\"sigmoid\")\n",
    "        >>> z = sigmoid_layer(a)\n",
    "        >>> gradient = sigmoid_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, activation_type = \"sigmoid\"):\n",
    "        \"\"\"\n",
    "            TODO: Initialize activation type and placeholders here.\n",
    "        \"\"\"\n",
    "        if activation_type not in [\"sigmoid\", \"tanh\", \"ReLU\", \"leakyReLU\"]:\n",
    "            raise NotImplementedError(f\"{activation_type} is not implemented.\")\n",
    "\n",
    "        # Type of non-linear activation.\n",
    "        self.activation_type = activation_type\n",
    "\n",
    "        # Placeholder for input. This will be used for computing gradients.\n",
    "        self.x = None\n",
    "        \n",
    "        # Cache the output computed here, it's easier to compute the gradient using this.\n",
    "        self.activation_result = None\n",
    "        \n",
    "    def show(self):\n",
    "        print(\"Layer = {}\".format(self.activation_type))\n",
    "\n",
    "    def __call__(self, a):\n",
    "        '''\n",
    "            This method allows your instances to be callable.\n",
    "        '''\n",
    "        return self.forward(a)\n",
    "\n",
    "    def forward(self, a):\n",
    "        '''\n",
    "            Compute the forward pass gradients\n",
    "        '''\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.sigmoid(a)\n",
    "        \n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.tanh(a)\n",
    "        \n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.ReLU(a)\n",
    "        \n",
    "        elif self.activation_type == \"leakyReLU\":\n",
    "            return self.leakyReLU(a)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            grad = self.grad_sigmoid()\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            grad = self.grad_tanh()\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            grad = self.grad_ReLU()\n",
    "            \n",
    "        elif self.activation_type == \"leakyReLU\":\n",
    "            grad = self.grad_leakyReLU()\n",
    "            \n",
    "        return grad * delta\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        self.x = x\n",
    "        self.activation_result = (1/(1 + np.exp(-1*x)))\n",
    "        return self.activation_result\n",
    "\n",
    "    def tanh(self, x):\n",
    "        self.x = x\n",
    "        self.activation_result = np.tanh(x)\n",
    "        return self.activation_result\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        self.x = x\n",
    "        self.activation_result = np.maximum(0, x)\n",
    "        return self.activation_result\n",
    "\n",
    "    def leakyReLU(self, x):\n",
    "        self.x = x\n",
    "        self.activation_result = np.maximum(0.1*x, x)\n",
    "        return self.activation_result\n",
    "\n",
    "    def grad_sigmoid(self):\n",
    "        return self.activation_result * (1 - self.activation_result)\n",
    "\n",
    "    def grad_tanh(self):\n",
    "        return (1 - (self.activation_result * self.activation_result))\n",
    "\n",
    "    def grad_ReLU(self):\n",
    "        return np.where(self.x > 0, 1, 0)\n",
    "\n",
    "    def grad_leakyReLU(self):\n",
    "        return np.where(self.x > 0, 1, 0.1)\n",
    "        \n",
    "class Layer():\n",
    "    \"\"\"\n",
    "        This class implements Fully Connected layers for your neural network.\n",
    "\n",
    "        Example:\n",
    "            >>> fully_connected_layer = Layer(784, 100)\n",
    "            >>> output = fully_connected_layer(input)\n",
    "            >>> gradient = fully_connected_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_units, out_units, debug_mode = False):\n",
    "        \"\"\"\n",
    "            Define the architecture and create placeholder.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.w = np.random.randn(in_units, out_units)\n",
    "        self.b = np.random.randn(1, out_units)\n",
    "        \n",
    "        self.x = None\n",
    "        self.a = None\n",
    "\n",
    "        self.d_x = None\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "        \n",
    "        self.d_w_tminus1 = 0 # Don't initialize to d_w\n",
    "        self.d_b_tminus1 = 0 # Don't initialize to d_b\n",
    "        \n",
    "        self.w_best = self.w # Used to store the best weights\n",
    "        self.b_best = self.b # Used to store the best biases\n",
    "        \n",
    "        # Used by show()\n",
    "        self.in_units = in_units\n",
    "        self.out_units = out_units\n",
    "        \n",
    "    def show(self):\n",
    "        \"\"\"\n",
    "            show() is used to display the details of the \n",
    "            Helpful for debugging\n",
    "        \"\"\"\n",
    "        print(\"Input and output of the layer = {} and {}\".format(self.in_units, self.out_units))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "            Make layer callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Compute the forward pass output here\n",
    "            DO NOT apply activation here.\n",
    "            Return self.a\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        \n",
    "        self.a = np.dot(x, self.w) + self.b\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta, l2_penalty=0,debug_mode=False):\n",
    "        \"\"\"\n",
    "            Backward pass code.\n",
    "            Algo - This takes in gradient from its next layer as input,\n",
    "            computes gradient for its weights and the delta to pass to its previous layers.\n",
    "            Return self.dx\n",
    "        \"\"\"\n",
    "        numSamples = self.x.shape[0]\n",
    "        if(debug_mode):\n",
    "            print(\"backward: Shape of delta = {}\".format(delta.shape))\n",
    "        self.d_x = 1*(np.dot(delta, self.w.transpose()) / (numSamples))\n",
    "        self.d_w = 1*(np.dot(self.x.transpose(), delta) / (numSamples) - l2_penalty * self.w)\n",
    "        self.d_b = 1*(np.sum(delta, axis=0) / (numSamples))\n",
    "        \n",
    "        return self.d_x\n",
    "    \n",
    "    def update_weights_biases(self, eta, weightIndex1, weightIndex2, biasIndex, isBias):\n",
    "        \"\"\"\n",
    "            Compute the gradient using numerical approximation\n",
    "            Args:\n",
    "                epsilon = The amount by which we modify the weight\n",
    "                weightLayer = The layer of the weight (originating layer)\n",
    "                weightIndex1, weightIndex2 = Both indices required to compute the weight\n",
    "                biasIndex - If this is a bias weight, what index should be used?\n",
    "                isBias = Is it a bias weight or the synapse between two neurons?\n",
    "        \"\"\"\n",
    "        # Perform forward pass with weights = w + epsilon\n",
    "        if(isBias):\n",
    "            self.b[0, biasIndex] += eta\n",
    "        else:\n",
    "            self.w[weightIndex1][weightIndex2] += eta\n",
    "    \n",
    "    def get_weights_biases(self, weightIndex1=0, weightIndex2=0, biasIndex=0, isBias=True):\n",
    "        \"\"\"\n",
    "            Here, we could compute the values of weights and biases\n",
    "            Args:\n",
    "                weightIndex1, weightIndex2 = Both indices required to compute the weight\n",
    "                biasIndex - If this is a bias weight, what index should be used?\n",
    "                isBias = Is it a bias weight or the synapse between two neurons?\n",
    "        \"\"\"\n",
    "        return self.w[weightIndex1][weightIndex2], self.b[0,biasIndex]\n",
    "    \n",
    "    def get_dw_db(self, weightIndex1=0, weightIndex2=0, biasIndex=0, isBias=True):\n",
    "        \"\"\"\n",
    "            Getter method to return the values of the values and the variables\n",
    "        \"\"\"\n",
    "        return self.d_w, self.d_b\n",
    "        \n",
    "\n",
    "    def modify_parameters(self, learning_rate, useMomentum=False, momentumGamma = 0.9):\n",
    "        \"\"\"\n",
    "            This function is used to modify the parameters of a layer based on \n",
    "            learning rate, momentum if it's enabled\n",
    "        \"\"\"\n",
    "        if useMomentum:\n",
    "            delta_w = (learning_rate * self.d_w) + momentumGamma * self.d_w_tminus1\n",
    "            delta_b = (learning_rate * self.d_b) + momentumGamma * self.d_b_tminus1\n",
    "            \n",
    "            self.w += delta_w\n",
    "            self.b += delta_b\n",
    "            \n",
    "            self.d_w_tminus1 = delta_w\n",
    "            self.d_b_tminus1 = delta_b\n",
    "        else:\n",
    "            self.w += learning_rate * self.d_w\n",
    "            self.b += learning_rate * self.d_b\n",
    "        \n",
    "    def store_old_parameters(self):\n",
    "        \"\"\"\n",
    "            Used to store the old parameters so that calculation of accurate delta\n",
    "            is achieved when using momentum\n",
    "        \"\"\"\n",
    "        self.w_best = self.w\n",
    "        self.b_best = self.b\n",
    "        \n",
    "    def restore_old_parameters(self):\n",
    "        \"\"\"\n",
    "            Restore the older parameters\n",
    "        \"\"\"\n",
    "        self.w = self.w_best\n",
    "        self.b = self.b_best\n",
    "        \n",
    "    \n",
    "class Neuralnetwork():\n",
    "    \"\"\"\n",
    "    Create a Neural Network specified by the input configuration.\n",
    "\n",
    "    Example:\n",
    "        >>> net = NeuralNetwork(config)\n",
    "        >>> output = net(input)\n",
    "        >>> net.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, debug_mode=False):\n",
    "        \"\"\"\n",
    "            Create the Neural Network using config.\n",
    "        \"\"\"\n",
    "        self.layers = []             # Store all layers in this list.\n",
    "        self.x = None                # Save the input to forward in this\n",
    "        self.y = None                # Save the output vector of model in this\n",
    "        self.targets = None          # Save the targets in forward in this variable\n",
    "        self.l2_penalty = None       # L2 penalty required\n",
    "        self.debug_mode = debug_mode # Used for debug prints\n",
    "        # Add layers specified by layer_specs.\n",
    "        for i in range(len(config['layer_specs']) - 1):\n",
    "            self.layers.append(Layer(config['layer_specs'][i], config['layer_specs'][i+1]))\n",
    "            if i < len(config['layer_specs']) - 2:\n",
    "                self.layers.append(Activation(config['activation']))\n",
    "        if(self.debug_mode):\n",
    "            print(self.layers)\n",
    "\n",
    "    def __call__(self, x, targets=None):\n",
    "        \"\"\"\n",
    "            Make NeuralNetwork callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x, targets)\n",
    "\n",
    "    def forward(self, x, targets=None, l2_penalty=0):\n",
    "        \"\"\"\n",
    "            TODO: Compute forward pass through all the layers in the network and return it.\n",
    "            If targets are provided, return loss as well.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.targets = targets\n",
    "        \n",
    "        prevLayer = x\n",
    "        for layer in self.layers:\n",
    "            if(self.debug_mode):\n",
    "                print(\"Layer = \".format(layer.show()))\n",
    "            prevLayer = layer.forward(prevLayer)\n",
    "            \n",
    "        # Computing y\n",
    "        self.y = softmax(prevLayer)\n",
    "        \n",
    "        # Computing loss\n",
    "        loss = self.loss(self.y, targets)\n",
    "        \n",
    "        if l2_penalty:\n",
    "            for layer in self.layers:\n",
    "                if (type(layer) != str):\n",
    "                    loss += 0.5 * l2_penalty * np.sum(layer.w ** 2)\n",
    "        return loss\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        '''\n",
    "            TODO: compute the categorical cross-entropy loss and return it.\n",
    "        '''\n",
    "        numSamples = logits.shape[0]\n",
    "        epsilon = 1e-30\n",
    "        return -1 * np.sum(np.multiply(targets, np.log(logits + epsilon))) / numSamples\n",
    "    \n",
    "    def backward(self, l2_penalty=0):\n",
    "        '''\n",
    "            TODO: Implement backpropagation here.\n",
    "            Call backward methods of individual layers.\n",
    "        '''\n",
    "        if(self.debug_mode):\n",
    "            print(\"backward: shape of y = {}\".format(self.y.shape))\n",
    "        delta = (self.targets - self.y)/(self.y.shape[1])\n",
    "        for layer in self.layers[::-1]:\n",
    "            if(isinstance(layer, Layer)):\n",
    "                delta = layer.backward(delta, l2_penalty)\n",
    "            else:\n",
    "                delta = layer.backward(delta)\n",
    "    \n",
    "    def modify_parameters(self, learningRate, useMomentum=False, momentumGamma=0.9):\n",
    "        \"\"\"\n",
    "            This calls the modify parameters for each layer during the backward pass\n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            if(isinstance(layer, Layer)):\n",
    "                layer.modify_parameters(learningRate, useMomentum, momentumGamma)\n",
    "    \n",
    "    def store_old_parameters(self):\n",
    "        \"\"\"\n",
    "            store all the parameters for all the layers\n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            if(isinstance(layer, Layer)):\n",
    "                layer.store_old_parameters()\n",
    "    \n",
    "    def restore_old_parameters(self):\n",
    "        \"\"\"\n",
    "            Restore all the parameters back into self.w, self.b for all the layers\n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            if(isinstance(layer, Layer)):\n",
    "                layer.restore_old_parameters()\n",
    "    \n",
    "    def predict(self, x, y):\n",
    "        \"\"\"\n",
    "            Given input dataset x, predict the outputs for x using the learned parameters\n",
    "            After computing the learned parameters, compute the losses required\n",
    "        \"\"\"\n",
    "        prevLayer = x\n",
    "        for layer in self.layers:\n",
    "            prevLayer = layer.forward(prevLayer)\n",
    "        \n",
    "        predictions = np.argmax(softmax(prevLayer), axis=1)\n",
    "        targets = np.argmax(y, axis=1)\n",
    "        return np.mean(targets == predictions)\n",
    "    \n",
    "    def update_weights_biases(self, eta, layerNumber, weightIndex1, \\\n",
    "                              weightIndex2, biasIndex, isBias=True):\n",
    "        \"\"\"\n",
    "            Call weight and bias update for specific weights and biases in the network\n",
    "        \"\"\"\n",
    "        layer = self.layers[layerNumber]\n",
    "        if (isinstance(layer, Layer)):\n",
    "            layer.update_weights_biases(eta, weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "    \n",
    "    def get_weights_biases(self, layerNumber, weightIndex1, weightIndex2, \\\n",
    "                          biasIndex, isBias=True):\n",
    "        \"\"\"\n",
    "            Here, we will compute the weights and biases for the network\n",
    "        \"\"\"\n",
    "        layer = self.layers[layerNumber]\n",
    "        w, b = None, None\n",
    "        if (isinstance(layer, Layer)):\n",
    "            w, b = layer.get_weights_biases(weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "        return w, b\n",
    "    \n",
    "    def get_dw_db(self, layerNumber, weightIndex1, weightIndex2, \\\n",
    "                          biasIndex, isBias=True):\n",
    "        \"\"\"\n",
    "            Here, we will compute the weights and biases for the network\n",
    "        \"\"\"\n",
    "        layer = self.layers[layerNumber]\n",
    "        dw, db = None, None\n",
    "        if (isinstance(layer, Layer)):\n",
    "            dw, db = layer.get_dw_db(weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "        return dw, db\n",
    "\n",
    "def miniBatchGenerator(x, y, batchSize, debug_mode = False):\n",
    "    \"\"\"\n",
    "        A generator that's used to generate mini batches for the given dataset\n",
    "        Inputs: x - dataset, y - targets, batchSize - Number of samples in each batch\n",
    "        Generator function: Simply yields the values generated from x and y\n",
    "    \"\"\"\n",
    "    if(debug_mode):\n",
    "        print(\"Shape of x and y are {}, {}\".format(x.shape, y.shape))\n",
    "    new_data = np.zeros((x.shape[0], (x.shape[1] + y.shape[1])))\n",
    "    for i in range(x.shape[0]):\n",
    "        new_data[i][:-10] = x[i]\n",
    "        new_data[i][-10:] = y[i]\n",
    "    \n",
    "    if(debug_mode):\n",
    "        print(\"miniBatchGenerator: Shape of new_data = {}\".format(new_data.shape))\n",
    "    \n",
    "    random.shuffle(new_data)\n",
    "    \n",
    "    numSets = int(x.shape[0] / batchSize) if (x.shape[0] % batchSize == 0) else int(x.shape[0] / batchSize) + 1\n",
    "    \n",
    "    if(debug_mode):\n",
    "        print(\"miniBatchGenerator: x.shape = {}, new_data.shape = {}, Number of sets = {}, batchSize = {}\".\\\n",
    "              format(x.shape, new_data.shape, numSets, batchSize))\n",
    "    \n",
    "    for i in range(numSets):\n",
    "        startIdx, endIdx = i * batchSize, min((i+1) * batchSize, x.shape[0])\n",
    "        if(debug_mode):\n",
    "            print(\"startIdx = {}, endIdx = {}\".format(startIdx, endIdx))\n",
    "        data, targets = new_data[startIdx:endIdx, :-10], new_data[startIdx:endIdx, -10:]\n",
    "        if(debug_mode):\n",
    "            print(\"Shape of data, targets are {} and {}\".format(data.shape, targets.shape))\n",
    "        yield data, targets\n",
    "        \n",
    "def train(model, x_train, y_train, x_valid, y_valid, config):\n",
    "    \"\"\"\n",
    "        TODO: Train your model here.\n",
    "        Implement batch SGD to train the model.\n",
    "        Implement Early Stopping.\n",
    "        Use config to set parameters for training like learning rate, momentum, etc.\n",
    "    \"\"\"\n",
    "    # Parse configurational settings\n",
    "    numEpochs = config['epochs']\n",
    "    earlyStopping = config['early_stop']\n",
    "    useMomentum = config['momentum']\n",
    "    L2Penalty = config['L2_penalty']\n",
    "    momentumGamma = config['momentum_gamma']\n",
    "    batchSize = config['batch_size']\n",
    "    learningRate = config['learning_rate']\n",
    "    earlyStop = config['early_stop']\n",
    "    earlyStopEpoch = config['early_stop_epoch']\n",
    "    #debug_mode = config['debug_mode']\n",
    "    debug_mode = False\n",
    "    # Variables for early stopping\n",
    "    minValidationLoss = float('inf')\n",
    "    currentIncreasingEpoch = 0\n",
    "    \n",
    "    # Results dict\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        trainLosses, trainAcc = [], []\n",
    "        # TODO: Here, we implement Mini Batch Gradient Descent\n",
    "        for data, targets in miniBatchGenerator(x_train, y_train, batchSize):\n",
    "            trainingLoss = model.forward(data, targets, L2Penalty)\n",
    "            trainLosses.append(trainingLoss)\n",
    "            model.backward(l2_penalty=L2Penalty)\n",
    "            model.modify_parameters(learningRate, useMomentum, momentumGamma)\n",
    "            trainingAccuracy = model.predict(data, targets)\n",
    "            trainAcc.append(trainingAccuracy)\n",
    "            \n",
    "        trainLoss = np.mean(np.array(trainLosses))\n",
    "        trainAcc = np.mean(np.array(trainAcc))\n",
    "        validationLoss = model.forward(x_valid,y_valid, L2Penalty)\n",
    "        validationAcc = model.predict(x_valid, y_valid)\n",
    "        \n",
    "\n",
    "        print(\"train: Epoch = {}, training loss = {}, training acc = {}, \\\n",
    "        validation loss = {}, validation acc = {}\".format(epoch, trainLoss, trainAcc, validationLoss, validationAcc))\n",
    "        \n",
    "        # Results dictionary contains a list of values for each epoch\n",
    "        results['epoch'].append(epoch+1)\n",
    "        results['trainloss'].append(trainLoss)\n",
    "        results['trainAcc'].append(trainAcc)\n",
    "        results['validationLoss'].append(validationLoss)\n",
    "        results['validationAcc'].append(validationAcc)\n",
    "\n",
    "        # Early stopping\n",
    "        if validationLoss < minValidationLoss:\n",
    "            model.store_old_parameters()\n",
    "            minValidationLoss = validationLoss\n",
    "            currentIncreasingEpoch = 0\n",
    "        else:\n",
    "            currentIncreasingEpoch += 1\n",
    "        \n",
    "        if earlyStop:\n",
    "            if currentIncreasingEpoch > earlyStopEpoch:\n",
    "                break\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "        TODO: Calculate and return the accuracy on the test set.\n",
    "    \"\"\"\n",
    "    return model.predict(X_test, y_test)\n",
    "\n",
    "def load_q3b_data(x_train, y_train, debug_mode=True):\n",
    "    \"\"\"\n",
    "        Loading the dataset for question 3(b)\n",
    "        Args:\n",
    "            x_train - X (60000 x 784 values)\n",
    "            y_train - Y (1 x 10 values)\n",
    "        Returns:\n",
    "            x_small - subset of x_train\n",
    "            y_small - subset of y_train\n",
    "    \"\"\"\n",
    "    x_small, y_small = [], []\n",
    "\n",
    "    sample_indices = [0] * 10\n",
    "    for i in range(10):\n",
    "        for j in range(x_train.shape[0]):\n",
    "            if(y_train[j][i] == 1):\n",
    "                sample_indices[i] = j\n",
    "                # x_small, y_small results in the following values\n",
    "                x_small.append(x_train[j])\n",
    "                y_small.append(y_train[j])\n",
    "                if(debug_mode):\n",
    "                    print(\"For sample type {}, index = {}\".format(i, j))\n",
    "                break\n",
    "    \n",
    "    return np.array(x_small), np.array(y_small)\n",
    "\n",
    "def validation_split(x, y, debug_mode=False):\n",
    "    \"\"\"\n",
    "        Computes a 20% split after performing a random shuffle\n",
    "    \"\"\"\n",
    "    new_data = np.zeros((x.shape[0], (x.shape[1] + y.shape[1])))\n",
    "    if(debug_mode):\n",
    "        print(\"Shape of new_data = {}\".format(new_data.shape))\n",
    "    for i in range(x.shape[0]):\n",
    "        new_data[i][:-10] = x[i]\n",
    "        new_data[i][-10:] = y[i]\n",
    "    \n",
    "    random.shuffle(new_data)\n",
    "    length = len(new_data)\n",
    "    split = int(length * 0.8)\n",
    "    x_train = new_data[:split, : -10]\n",
    "    y_train = new_data[:split,-10:]\n",
    "    x_valid = new_data[split:,:-10]\n",
    "    y_valid = new_data[split:,-10:]\n",
    "    \n",
    "    return (x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the configuration file here\n",
    "    config = load_config(\"./\")\n",
    "    print(\"Config = {}\".format(config))\n",
    "\n",
    "    # enable or disable the value for debug mode\n",
    "    debug_mode = False\n",
    "\n",
    "    # When this is set, only 3(b) results would be computed\n",
    "    run_q3b = False\n",
    "\n",
    "    # eta is used to update the weights. We use the value of eta computed here\n",
    "    eta = 0.01\n",
    "    \n",
    "    L2Penalty = config['L2_penalty']\n",
    "    learningRate = config['learning_rate']\n",
    "    useMomentum = config['momentum']\n",
    "    momentumGamma = config['momentum_gamma']\n",
    "    \n",
    "    # Create the model\n",
    "    model  = Neuralnetwork(config, debug_mode)\n",
    "\n",
    "    # Load the data\n",
    "    x_train, y_train = load_data(path=\"./\", mode=\"train\")\n",
    "    x_test,  y_test  = load_data(path=\"./\", mode=\"t10k\")\n",
    "    print(\"Shape of x_train = {}, y_train = {}\".format(x_train.shape, y_train.shape))\n",
    "    \n",
    "    if(run_q3b == False):\n",
    "        x_train, y_train, x_valid, y_valid = validation_split(x_train, y_train, debug_mode)\n",
    "        print(\"Shape of x_train = {}\".format(x_train.shape))\n",
    "        print(\"Shape of y_train = {}\".format(y_train.shape))\n",
    "        print(\"Shape of x_valid = {}\".format(x_valid.shape))\n",
    "        print(\"Shape of y_valid = {}\".format(y_valid.shape))\n",
    "    else:\n",
    "        x_small, y_small = load_q3b_data(x_train, y_train)\n",
    "        print(\"Shape of x_small = {} and y_small = {}\".format(x_small.shape, y_small.shape))\n",
    "        \n",
    "    # What should be done if run_q3b = False?\n",
    "    if(run_q3b == False):\n",
    "        # TODO: train the model\n",
    "        model, results = train(model, x_train, y_train, x_valid, y_valid, config)\n",
    "\n",
    "        # Restoring the best weights for the network\n",
    "        model.restore_old_parameters()\n",
    "\n",
    "        # Compute the test accuracy\n",
    "        testAcc = test(model, x_test, y_test)\n",
    "        print(\"Test accuracy is {}\".format(testAcc))\n",
    "    else:\n",
    "        weightIndices = [(0, 0), (0, 0), (0, 0), (2, 3), (4, 5), (11, 4), (5, 10)]\n",
    "        biasIndices = [1, 1, 1, 0, 0, 0, 0]\n",
    "        isBias = False\n",
    "        for i in range(len(biasIndices)):\n",
    "            if i < 2:\n",
    "                isBias = True\n",
    "            else:\n",
    "                isBias = False\n",
    "            weightIndex1, weightIndex2 = weightIndices[i][0], weightIndices[i][1]\n",
    "            biasIndex = biasIndices[i]\n",
    "            # layers are FC, Act, FC, Act, FC\n",
    "            # Which amongst the following layers must be chosen?\n",
    "            layerNumber = 0\n",
    "            # First one should be at the softmax layer\n",
    "            if i == 0:\n",
    "                layerNumber = 4\n",
    "            elif i == 1:\n",
    "                layerNumber = 2\n",
    "            elif i == 2:\n",
    "                layerNumber = 4\n",
    "            elif (i == 3 or i == 4):\n",
    "                layerNumber = 4\n",
    "            elif (i == 5 or i == 6):\n",
    "                layerNumber = 0\n",
    "            \n",
    "            # Update weights and biases for the given network\n",
    "            model.update_weights_biases(eta, layerNumber, weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "            # Forward pass through the networks\n",
    "            trainingLossPlusEta = model.forward(x_small, y_small, L2Penalty)\n",
    "            model.update_weights_biases(-2*eta, layerNumber, weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "            trainingLossMinusEta = model.forward(x_small, y_small, L2Penalty)\n",
    "            print(\"plus eta loss, minus eta loss = {}, {}\".format(trainingLossPlusEta, trainingLossMinusEta))\n",
    "            numericalDerivative = (trainingLossPlusEta - trainingLossMinusEta)/(2 * x_small.shape[0] * eta)\n",
    "            \n",
    "            # Reset the weights and biases to the original state\n",
    "            model.update_weights_biases(eta, layerNumber, weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "            \n",
    "            trainingLossBackProp = model.forward(x_small, y_small, L2Penalty)\n",
    "            model.backward(l2_penalty=L2Penalty)\n",
    "            model.modify_parameters(learningRate, useMomentum, momentumGamma)\n",
    "            \n",
    "            dw, db = model.get_dw_db(layerNumber, weightIndex1, weightIndex2, biasIndex, isBias)\n",
    "            #print(\"Shapes of dw and db are {} and {}\".format(dw.shape, db.shape))\n",
    "            \n",
    "            if(isBias):\n",
    "                print(\"At index = {}, numericalDerivative, backPropDerivative are {}, {}\".\\\n",
    "                      format(i, numericalDerivative, np.sum(db[biasIndex], axis=0)))\n",
    "            else:\n",
    "                print(\"At index = {}, numericalDerivative, backPropDerivative are {}, {}\".\\\n",
    "                      format(i, numericalDerivative, dw[weightIndex1][weightIndex2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running checker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Forward Pass is CORRECT\n",
      "Tanh Forward Pass is CORRECT\n",
      "ReLU Forward Pass is CORRECT\n",
      "leakyReLU Forward Pass is CORRECT\n",
      "-------------------- \n",
      "\n",
      "Sigmoid Gradient is CORRECT\n",
      "Tanh Gradient is CORRECT\n",
      "ReLU Gradient is CORRECT\n",
      "leakyReLU Gradient is CORRECT\n",
      "-------------------- \n",
      "\n",
      "Layer1: Input is CORRECT\n",
      "Layer1: Weights is CORRECT\n",
      "Layer1: Biases is CORRECT\n",
      "Layer1: Weight Gradient is CORRECT\n",
      "Layer1: Bias Gradient is CORRECT\n",
      "Layer2: Input is CORRECT\n",
      "Layer2: Weights is CORRECT\n",
      "Layer2: Biases is CORRECT\n",
      "Layer2: Weight Gradient is CORRECT\n",
      "Layer2: Bias Gradient is CORRECT\n",
      "Layer3: Input is CORRECT\n",
      "Layer3: Weights is CORRECT\n",
      "Layer3: Biases is CORRECT\n",
      "Layer3: Weight Gradient is CORRECT\n",
      "Layer3: Bias Gradient is CORRECT\n",
      "-------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CSE 253: Programming Assignment 2\n",
    "# Code snippet by Manjot Bilkhu\n",
    "# Winter 2020\n",
    "################################################################################\n",
    "# We've provided you with the dataset in PA2.zip\n",
    "################################################################################\n",
    "# To install PyYaml, refer to the instructions for your system:\n",
    "# https://pyyaml.org/wiki/PyYAMLDocumentation\n",
    "################################################################################\n",
    "# If you don't have NumPy installed, please use the instructions here:\n",
    "# https://scipy.org/install.html\n",
    "################################################################################\n",
    "\n",
    "import neuralnet\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    \"\"\"\n",
    "    Load the sanity data to verify your implementation.\n",
    "    \"\"\"\n",
    "    return pickle.load(open(path + 'sanity.pkl', 'rb'))\n",
    "\n",
    "\n",
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Load the configuration from config.yaml.\n",
    "    \"\"\"\n",
    "    return yaml.load(open('config.yaml', 'r'), Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "def check_error(error, msg):\n",
    "    \"\"\"\n",
    "    Verify that error is below the threshold.\n",
    "    \"\"\"\n",
    "    if error < 1e-6:\n",
    "        print(f\"{msg} is CORRECT\")\n",
    "    else:\n",
    "        print(f\"{msg} is WRONG\")\n",
    "\n",
    "\n",
    "def sanity_layers(data):\n",
    "    \"\"\"\n",
    "    Check implementation of the forward and backward pass for all activations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the seed to reproduce results.\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Pseudo-input.\n",
    "    random_input = np.random.randn(1, 50)\n",
    "\n",
    "    # Get the activations.\n",
    "    act_sigmoid = neuralnet.Activation('sigmoid')\n",
    "    act_tanh    = neuralnet.Activation('tanh')\n",
    "    act_ReLU    = neuralnet.Activation('ReLU')\n",
    "    act_leakyReLU = neuralnet.Activation('leakyReLU')\n",
    "\n",
    "    # Get the outputs for forward-pass.\n",
    "    out_sigmoid = act_sigmoid(random_input)\n",
    "    out_tanh    = act_tanh(random_input)\n",
    "    out_ReLU    = act_ReLU(random_input)\n",
    "    out_leakyReLU = act_leakyReLU(random_input)\n",
    "\n",
    "    # Compute the errors.\n",
    "    err_sigmoid = np.sum(np.abs(data['out_sigmoid'] - out_sigmoid))\n",
    "    err_tanh    = np.sum(np.abs(data['out_tanh'] - out_tanh))\n",
    "    err_ReLU    = np.sum(np.abs(data['out_ReLU'] - out_ReLU))\n",
    "    err_leakyReLU = np.sum(np.abs(data['out_leakyReLU'] - out_leakyReLU))\n",
    "\n",
    "    # Check the errors.\n",
    "    check_error(err_sigmoid, \"Sigmoid Forward Pass\")\n",
    "    check_error(err_tanh,    \"Tanh Forward Pass\")\n",
    "    check_error(err_ReLU,    \"ReLU Forward Pass\")\n",
    "    check_error(err_leakyReLU,    \"leakyReLU Forward Pass\")\n",
    "\n",
    "    print(20 * \"-\", \"\\n\")\n",
    "\n",
    "    # Compute the gradients.\n",
    "    grad_sigmoid = act_sigmoid.backward(1.0)\n",
    "    grad_tanh    = act_tanh.backward(1.0)\n",
    "    grad_ReLU    = act_ReLU.backward(1.0)\n",
    "    grad_leakyReLU = act_leakyReLU.backward(1.0)\n",
    "\n",
    "    # Compute the errors.\n",
    "    err_sigmoid_grad = np.sum(np.abs(data['grad_sigmoid'] - grad_sigmoid))\n",
    "    err_tanh_grad    = np.sum(np.abs(data['grad_tanh'] - grad_tanh))\n",
    "    err_ReLU_grad    = np.sum(np.abs(data['grad_ReLU'] - grad_ReLU))\n",
    "    err_leakyReLU_grad = np.sum(np.abs(data['grad_leakyReLU'] - grad_leakyReLU))\n",
    "\n",
    "    # Check the errors.\n",
    "    check_error(err_sigmoid_grad, \"Sigmoid Gradient\")\n",
    "    check_error(err_tanh_grad,    \"Tanh Gradient\")\n",
    "    check_error(err_ReLU_grad,    \"ReLU Gradient\")\n",
    "    check_error(err_leakyReLU_grad, \"leakyReLU Gradient\")\n",
    "\n",
    "    print(20 * \"-\", \"\\n\")\n",
    "\n",
    "\n",
    "def sanity_network(data, default_config):\n",
    "    \"\"\"\n",
    "    Check implementation of the neural network's forward pass and backward pass.\n",
    "    \"\"\"    \n",
    "    # Set seed to reproduce results.\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Random input for our network.\n",
    "    random_image = np.random.randn(1, 784)\n",
    "\n",
    "    # Initialize the network using the default configuration\n",
    "    nnet = neuralnet.Neuralnetwork(default_config)\n",
    "\n",
    "    # Compute the forward pass.\n",
    "    nnet(random_image, targets = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "\n",
    "    # Compute the backward pass.\n",
    "    nnet.backward()\n",
    "\n",
    "    layer_no = 0\n",
    "    for layer_idx, layer in enumerate(nnet.layers):\n",
    "        if isinstance(layer, neuralnet.Layer):\n",
    "            layer_no += 1\n",
    "            error_x   = np.sum(np.abs(data['nnet'].layers[layer_idx].x   - layer.x))\n",
    "            # Weights\n",
    "            error_w   = np.sum(np.abs(data['nnet'].layers[layer_idx].w   - layer.w))\n",
    "            error_b   = np.sum(np.abs(data['nnet'].layers[layer_idx].b   - layer.b))\n",
    "            error_d_w = np.sum(np.abs(data['nnet'].layers[layer_idx].d_w + layer.d_w))\n",
    "            error_d_b = np.sum(np.abs(data['nnet'].layers[layer_idx].d_b + layer.d_b))\n",
    "            \n",
    "            check_error(error_x,   f\"Layer{layer_no}: Input\")\n",
    "            check_error(error_w,   f\"Layer{layer_no}: Weights\")\n",
    "            check_error(error_b,   f\"Layer{layer_no}: Biases\")\n",
    "            check_error(error_d_w, f\"Layer{layer_no}: Weight Gradient\")\n",
    "            check_error(error_d_b, f\"Layer{layer_no}: Bias Gradient\")\n",
    "\n",
    "    print(20 * \"-\", \"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the data and configuration.\n",
    "    sanity_data    = get_data(\"./\")\n",
    "    default_config = load_config(\"./\")\n",
    "\n",
    "    # Run Sanity.\n",
    "    sanity_layers(sanity_data)\n",
    "    sanity_network(sanity_data, default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
