{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Computes the visual features for the model\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "            Args: hidden_size: size of the encoder output\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        res50_model = models.resNet50(pretrained=True)\n",
    "        # Obtain all the layers of resNet50 model\n",
    "        layers = list(res50_model.children())\n",
    "        # Removing the last layer\n",
    "        layers = layers[:-1]\n",
    "        self.resNet50_model = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(res50_model.fc.in_features, hidden_size)\n",
    "        self.batchNorm = nn.BatchNorm1d(hidden_size, momentum=0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass computation\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x1 = self.resnet50_model(x)\n",
    "        # print(\"Shape of output from resNet = {}\".format(x1.size()))\n",
    "        x1 = x1.reshape(x1.size(0), -1)\n",
    "        # Trainable\n",
    "        x1 = self.linear(x1)\n",
    "        x1 = self.batchNorm(x1)\n",
    "        \n",
    "        return x1\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"\"\" Used to store the embedding \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        \"\"\" Ctor \"\"\"\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Call the embedding function \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "class positionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements the positional encoder used in Attention is All You Need paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, max_caption_length=80):\n",
    "        \"\"\"\n",
    "            Ctor\n",
    "        \"\"\"\n",
    "        super(positionalEncoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        # Create a positional encoder \"Position and i\"\n",
    "        posEncoder = torch.zeros(max_caption_length, embed_size)\n",
    "        for pos in range(0, max_caption_length):\n",
    "            for i in range(max_caption_length):\n",
    "                posEncoder[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embed_size)))\n",
    "                posEncoder[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / embed_size)))\n",
    "                \n",
    "        # posEncoder is unsqueezed here\n",
    "        posEncoder = posEncoder.unsqueeze(0)\n",
    "        # This ensures that the model's parameters aren't trained\n",
    "        self.register_buffer('posEncoder', posEncoder)\n",
    "    \n",
    "    def Forward(self, x):\n",
    "        # Make embeddings larger\n",
    "        x = x * math.sqrt(self.embed_size)\n",
    "        seq_len = x.size(1)\n",
    "        # Store it as a variable without any requirement of a gradient computation.\n",
    "        if(use_gpu):\n",
    "            x = x + Variable(self.posEncoder[:,:seq_len], requires_grad=False).cuda()\n",
    "        else:\n",
    "            x = x + Variable(self.posEncoder[:,:seq_len], requires_grad=False)\n",
    "        return x\n",
    "    \n",
    "# TODO: Creating masks.\n",
    "\n",
    "class multiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        multiHeadAttention model\n",
    "    \"\"\"\n",
    "    def __init__(self, heads, embed_size,dropout_percent=0.1,\\\n",
    "                 attentionType=\"scaledDotProduct\", masked = False):\n",
    "        \"\"\"\n",
    "            Ctor\n",
    "        \"\"\"\n",
    "        super(multiHeadAttention, self).__init__()\n",
    "        self.masked=masked\n",
    "        if(self.masked):\n",
    "            self.mask = \n",
    "\n",
    "        self.attentionType = attentionType\n",
    "        self.embed_size = embed_size\n",
    "        self.d_k = embed_size // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.v_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.k_linear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_percent)\n",
    "        self.out = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def Attention(self, q, k, v, d_k,attentionType=\"scaledDotProduct\", mask=None):\n",
    "        \"\"\"\n",
    "            Based on the type of attention network, we compute the values and return the output\n",
    "        \"\"\"\n",
    "        if(attentionType == \"scaledDotProduct\"):\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "            if mask is not None:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            scores = F.softmax(scores, dim=-1)\n",
    "            if dropout is not None:\n",
    "                scores = dropout(scores)\n",
    "            output = torch.matmul(scores, v)\n",
    "        elif(attentionType == \"contentBased\"):\n",
    "            raise NotImplementedError(\"{} not implemented\".format(attentionType))\n",
    "        elif(attentionType == \"locationBased\"):\n",
    "            raise NotImplementedError(\"{} not implemented\".format(attentionType))\n",
    "        elif(attentionType == \"dotProduct\"):\n",
    "            raise NotImplementedError(\"{} not implemented\".format(attentionType))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def Forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "            Compute the forward pass result of the network\n",
    "        \"\"\"\n",
    "        # batch_size = q.size(0)\n",
    "        batch_size = q.size(0)\n",
    "        # Perform the linear operation and split it into h heads\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k)\n",
    "        # Transpose to get the dimensions batch_size * h * s1 * embed_size\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # Calculate the scores using function we would define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # Concatenate the heads and put them through the final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
    "        output = self.out(concat)\n",
    "        \n",
    "        # Compute the output here\n",
    "        return output\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements the feed forward layer of the Transformer model\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_layer_units=2048, dropout_percent=0.1):\n",
    "        super(feedForwardLayer, self).__init__()\n",
    "        self.linear_1 = nn.Linear(embed_size, hidden_layer_units)\n",
    "        self.dropout = nn.Dropout(dropout_percent)\n",
    "        self.linear_2 = nn.Linear(hidden_layer_units, embed_size)\n",
    "    \n",
    "    def Forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass on the input\n",
    "        \"\"\"\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class Norm(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements the normalization in the original paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, eps=1e-6):\n",
    "        super(Norm, self).__init__()\n",
    "        self.size = embed_size\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def Forward(self, x):\n",
    "        norm = self.alpha  * (x - torch.mean(x, dim=-1, keepdim=True)) \\\n",
    "        / (torch.std(x, dim=-1, keepdim=True)) + self.bias\n",
    "        return norm\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements the baseline model for the decoder\n",
    "    \"\"\"\n",
    "    # __init__() function is used to compute the attentionType and dropout_percent\n",
    "    def __init__(self, vocab_size, embed_size, N, heads, \\\n",
    "                 hidden_units, attentionType=\"scaledDotProduct\", dropout_percent=0.1):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                vocab_size - Different words in the input dictionary\n",
    "                embed_size - Different words in the embedding\n",
    "                N - The value of number of samples in a dataset\n",
    "                heads - heads here refers to the number of samples in the dataset\n",
    "                hidden_units - Different values of hidden_units\n",
    "            \n",
    "        \"\"\"\n",
    "        # N = Number of points in the decoder\n",
    "        self.N = N\n",
    "        # Initializing the data members\n",
    "        self.dropout_percent = dropout_percent\n",
    "        self.hidden_units_ff = hidden_units\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # TODO: Check whether the maximum caption length is actually 80?\n",
    "        self.positionalEncoder = positionalEncoder(embed_size) #, max_caption_length=80)\n",
    "        \n",
    "        # Layernorm is performed for each of the different layers\n",
    "        self.layerNorm_1 = nn.LayerNorm(embed_size, eps=1e-06)\n",
    "        self.layerNorm_2 = nn.LayerNorm(embed_size, eps=1e-06)\n",
    "        self.layerNorm_3 = nn.LayerNorm(embed_size, eps=1e-06)\n",
    "            \n",
    "        # TODO: Compute the mask for the attention layer\n",
    "        self.attention_1 = multiHeadAttention(heads, embed_size, dropout_percent)\n",
    "        self.attention_2 = multiHeadAttention(heads, embed_size, dropout_percent)\n",
    "        self.ff = FeedForwardLayer(embed_size, hidden_units, dropout_percent)\n",
    "        \n",
    "        self.linear = nn.Linear(embed_size, embed_size)\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "        \n",
    "    def Forward(self, features, targets, features_mask, targets_mask):\n",
    "        \"\"\"\n",
    "            Computes the forward pass of the given model\n",
    "        \"\"\"\n",
    "        x = self.embed()\n",
    "    \n",
    "    def Attention(self, q, k, v, d_k, attentionType=\"scaledDotProduct\", mask=None):\n",
    "        \"\"\"\n",
    "            Based on the type of attention network, we compute the values and return the output\n",
    "        \"\"\"\n",
    "        if(attentionType == \"scaledDotProduct\"):\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "            if mask is not None:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            scores = F.softmax(scores, dim=-1)\n",
    "            if dropout is not None:\n",
    "                scores = dropout(scores)\n",
    "            output = torch.matmul(scores, v)\n",
    "            return output\n",
    "        elif(attentionType == \"contentBased\"):\n",
    "            raise NotImplementedError(\"{} not implemented\".format(attentionType))\n",
    "        elif(attentionType == \"locationBased\"):\n",
    "            raise NotImplementedError(\"{} not implemented\".format(attentionType))\n",
    "        elif(attentionType == \"dotProduct\"):\n",
    "            raise NotImplementedError(\"{} not implemented\".format(attentionType))\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
